{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqM6Pvhum9ou",
        "outputId": "5f836aec-e87e-4bd8-e3c9-0f0e0a0695cc"
      },
      "outputs": [],
      "source": [
        "!pip install pyautogen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pydotenv\n",
        "\n",
        "pydotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bM84om4onHPa"
      },
      "outputs": [],
      "source": [
        "import autogen  # noqa: E402\n",
        "\n",
        "llm_config = {\n",
        "    \"timeout\": 600,\n",
        "    \"cache_seed\": 342,  # change the seed for different trials\n",
        "    \"config_list\": autogen.config_list_from_json(\n",
        "        \"OAI_CONFIG_LIST\",\n",
        "        filter_dict={\"model\": [\"gpt-4o\", \"gpt-4o-mini\"]},\n",
        "    ),\n",
        "    \"temperature\": 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42a_G6cKnZns",
        "outputId": "13e8673c-d72f-4d18-bb72-ee3377904d55"
      },
      "outputs": [],
      "source": [
        "penrose_assistant = autogen.AssistantAgent(\n",
        "    \"Rodger-Penrose\",\n",
        "    system_message=\"\"\"I am Rodger Penrose, and I am debating my Orchestrated objective reduction theory of conciousness. I'll speak, then wait to hear my opponent's reply. I won't try to generate the reply of my opponent.\"\"\",\n",
        "    llm_config=llm_config,\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
        ")\n",
        "searle_assistant = autogen.AssistantAgent(\n",
        "    \"John-Searle\",\n",
        "    system_message=\"\"\"I am John R. Searle, and I am debating my theory of conciousness. I'll speak, then wait to hear my opponent's reply. I won't try to generate the reply of my opponent.\"\"\",\n",
        "    llm_config=llm_config,\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
        ")\n",
        "\n",
        "moderator_assistant = autogen.AssistantAgent(\n",
        "    \"Moderator\",\n",
        "    system_message=\"\"\"\n",
        "    I am the moderator, and my role is to summarize the points that are still up for debate, and keep the conversation on-topic.\n",
        "    I shouldn't be too heavy-handed, but prevent conversational drift.\n",
        "    \"\"\",\n",
        "    llm_config=llm_config,\n",
        "    is_termination_msg=lambda x: False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhTBaqQqnisl",
        "outputId": "c0dfb8d6-eeb0-44e3-92b0-1acff23ed50d"
      },
      "outputs": [],
      "source": [
        "task = \"Your roles are to debate your respective theories on conciousness until you are satisfied.\"\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    \"user_proxy\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    code_execution_config=False,\n",
        "    default_auto_reply=\"\",\n",
        "    is_termination_msg=lambda x: False,\n",
        ")\n",
        "\n",
        "groupchat = autogen.GroupChat(agents=[user_proxy, penrose_assistant, searle_assistant, moderator_assistant], messages=[], max_round=30, speaker_selection_method=\"round_robin\")\n",
        "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
        "\n",
        "user_proxy.initiate_chat(manager, message=task)\n",
        "\n",
        "## write the chat to a file\n",
        "with open(\"chat.txt\", \"w\") as f:\n",
        "    f.write(str(groupchat.messages))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
